{{- if (and .Values.monitoring.enabled (ne .Values.monitoring.monitoringType "influxdb") .Values.monitoring.installDashboard) }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app.kubernetes.io/component: monitoring
    prometheus: OpenSearch-rules
    role: alert-rules
  name: prometheus-opensearch-service-rules
spec:
  groups:
    - name: {{ .Release.Namespace }}-{{ .Release.Name }}
      rules:
        - alert: OpenSearchCPULoadAlert
          annotations:
            description: 'OpenSearch CPU usage is above 95%.'
            summary: OpenSearch's CPU usage is above 95%
          expr: max(opensearch_process_cpu_percent{namespace="{{ .Release.Namespace }}"}) > 95
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        - alert: OpenSearchDiskUsageAlert
          annotations:
            description: 'OpenSearch disk usage is above 90%'
            summary: OpenSearch's disk usage is above 90%
          expr: 1 - sum(opensearch_fs_total_free_in_bytes{namespace="{{ .Release.Namespace }}"}) / sum(opensearch_fs_total_total_in_bytes{namespace="{{ .Release.Namespace }}"}) > 0.90
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        - alert: OpenSearchDiskTooMuchUsageAlert
          annotations:
            description: 'OpenSearch disk usage is above 98%'
            summary: OpenSearch's disk usage is above 98%
          expr: 1 - sum(opensearch_fs_total_free_in_bytes{namespace="{{ .Release.Namespace }}"}) / sum(opensearch_fs_total_total_in_bytes{namespace="{{ .Release.Namespace }}"}) > 0.98
          for: 3m
          labels:
            severity: critical
            namespace: {{ .Release.Namespace }}
        - alert: OpenSearchMemoryUsageAlert
          annotations:
            description: 'OpenSearch memory usage is above 95%.'
            summary: OpenSearch's memory usage is above 95%
          expr: max(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}", container="opensearch"}) / max(container_spec_memory_limit_bytes{namespace="{{ .Release.Namespace }}", container="opensearch"}) > 0.95
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        - alert: OpenSearchHeapMemoryUsageAlert
          annotations:
            description: 'OpenSearch heap memory usage is above 95%.'
            summary: OpenSearch's heap memory usage is above 95%
          expr: max(opensearch_jvm_mem_heap_used_percent{namespace="{{ .Release.Namespace }}"}) > 95
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        - alert: OpenSearchIsDegradedAlert
          annotations:
            description: 'OpenSearch is Degraded.'
            summary: Some of OpenSearch Service pods are down
          expr: opensearch_cluster_health_status_code{namespace="{{ .Release.Namespace }}"} == 6
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        - alert: OpenSearchIsDownAlert
          annotations:
            description: 'OpenSearch is Down.'
            summary: All of OpenSearch Service pods are down
          expr: opensearch_cluster_health_status_code{namespace="{{ .Release.Namespace }}"} == 10
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        {{- if (or .Values.dbaasAdapter.enabled .Values.elasticsearchDbaasAdapter.enabled) }}
        - alert: OpenSearchDBaaSIsDownAlert
          annotations:
            description: 'OpenSearch DBaaS agent is Down.'
            summary: OpenSearch DBaaS agent is Down
          expr: opensearch_dbaas_health_status{namespace="{{ .Release.Namespace }}"} == 1
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        {{- end }}
        {{- if .Values.curator.enabled }}
        - alert: OpenSearchLastBackupHasFailedAlert
          annotations:
            description: 'OpenSearch Last Backup Has Failed.'
            summary: OpenSearch Last Backup Has Failed
          expr: opensearch_backups_metric_last_backup_status{namespace="{{ .Release.Namespace }}"} != 1
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        {{- end }}
        {{- if (eq (include "opensearch.enableDisasterRecovery" .) "true") }}
        - alert: OpenSearchReplicationFailedAlert
          annotations:
            description: 'OpenSearch Replication has Failed.'
            summary: OpenSearch Replication has Failed
          expr: opensearch_replication_metric_status{namespace="{{ .Release.Namespace }}"} == 4
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        - alert: OpenSearchReplicationDegradedAlert
          annotations:
            description: 'OpenSearch Replication has Degraded.'
            summary: OpenSearch Replication has Degraded
          expr: opensearch_replication_metric_status{namespace="{{ .Release.Namespace }}"} == 2
          for: 3m
          labels:
            severity: warning
            namespace: {{ .Release.Namespace }}
        - alert: OpenSearchReplicationLeaderConnectionLostAlert
          annotations:
            description: 'OpenSearch Replication Leader connection lost.'
            summary: OpenSearch Replication Follower lost connection with Leader side
          expr: opensearch_replication_metric_status{namespace="{{ .Release.Namespace }}"} == -1
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        {{- if .Values.monitoring.thresholds.lagAlert }}
        - alert: OpenSearchReplicationTooHighLagAlert
          annotations:
            description: 'OpenSearch Replication has Index with too high Lag.'
            summary: OpenSearch Replication has Index with Lag higher than expected Maximum.
          expr: max(opensearch_replication_metric_index_lag{namespace="{{ .Release.Namespace }}"}) > {{ .Values.monitoring.thresholds.lagAlert }}
          for: 3m
          labels:
            severity: high
            namespace: {{ .Release.Namespace }}
        {{- end }}
        {{- end }}
{{- end }}
